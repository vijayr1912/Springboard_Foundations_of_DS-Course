---
title: "Building a Movie Recommendation System using R- Recommenderlab package"
author: "Vijay Raghunath, mentored by Amit Dingare"
date: "November, 2016"
output: html_notebook
---

## Project Overview

In this project, we develop a recommender system for recommending movies for the MovieLens database.

Initially the recommender system will be build using the colloborative filtering recommender (CFR) algorithm.

The basic idea of CFR systems is that, if two users share the same interests in the past, e.g. they liked the same book or the same movie, they will also have similar tastes in the future. If, for example, user A and user B have a similar purchase history and user A recently bought a book that user B has not yet seen, the basic idea is to propose this book to user B.

The collaborative filtering approach considers only user preferences and does not take into account the features or contents of the items (books or movies) being recommended.

## Used Libraries

The following libraries were used in this project:

```{r libs, warning=FALSE, error=FALSE, message=FALSE}
library(recommenderlab)
library(ggplot2)
library(tidyr)
library(data.table)
```

## UML class diagram for package recommenderlab
image: ![](recommenderlab-UML.png)

By using the algorithms available in Recommenderlab package, we can achieve 2-fold purpose:

* Predict the ratings for the movies that the user has not seen
* After having predicted all the ratings for the user, we can make a top-N recommendation for the users. This top-N recommendation list can be further drilled down for the genre preferred by the user.

### Recommenderlab Infrastructure
The package uses the abstract ratingMatrix to provide a common interface for rating data.
ratingMatrix implements many methods typically available for matrix-like objects.
For example, dim(), dimnames(), colCounts(), rowCounts(), colMeans(), rowMeans(), colSums()
and rowSums(). Additionally sample() can be used to sample from users (rows) and image()
produces an image plot.

For ratingMatrix we provide two concrete implementations realRatingMatrix and
binaryRatingMatrix to represent different types of rating matrices.
 
* realRatingMatrix - Implements a rating matrix with real valued ratings stored in sparse format defined in package Matrix. Sparse matrices in Matrix typically do not store 0s explicitly, however for realRatingMatrix we use these sparse matrices such that instead of 0s, NAs are not explicitly stored.

* binaryRatingMatrix - implements a 0-1 rating matrix using the implementation of itemMatrix
defined in package arules. itemMatrix stores only the ones and internally uses a sparse rep-
resentation from package Matrix. With this class structure recommenderlab can be easily
extended to other forms of rating matrices with different concepts for efficient storage in the
future.

### Exploring Parameters of Recommendation Models
```{r}
recommender_models <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
names(recommender_models)
lapply(recommender_models, "[[", "description")
```



## Dataset

The datasets for this project can be downloaded from the following site:
http://grouplens.org/datasets/movielens/latest.

There are two sets of data having different number of observations â€“

1. Small dataset - It contains 105339 ratings and 6138 tag applications across 10329 movies. These data were created by 668 users between April 03, 1996 and January 09, 2016. 

2. Large dataset- It contains 22884377 ratings and 586994 tag applications across 34208 movies. These data were created by 247753 users between January 09, 1995 and January 29, 2016.

For initial model building and validation, the smaller dataset is used.

The data are contained in four files: links.csv, movies.csv, ratings.csv and tags.csv. 

A brief description of the data files is as below - 

File Name | File Description
------------- | -------------
ratings.csv | All ratings are contained in the file `ratings.csv`. Each line of this file after                  the header row represents one rating of one movie by one user, and has the                       following format: `userId, movieId, rating, timestamp`.
movies.csv | Movie information is contained in the file `movies.csv`. Each line of this file                 after the header row represents one movie, and has the following format:                         `movieId, title, genres`. Movie titles are entered manually or imported from                     <https://www.themoviedb.org/>, and include the year of release in parentheses.Errors             and inconsistencies may exist in these titles.
links.csv | Identifiers that can be used to link to other sources of movie data are contained in             the file `links.csv`. Each line of this file after the header row represents one                 movie, and has the following format: `movieId, imdbId, tmdbId`.
tags.csv | All tags are contained in the file `tags.csv`. Each line of this file after the header             row represents one tag applied to one movie by one user, and has the following                   format: `userId, movieId, tag, timestamp`.

We only use the files movies.csv and ratings.csv to build a recommendation system. These files are used to build the user item ratings matrix, needed by the colloborative algorithms that are available with the *Recommenderlab* package.

```{r data_load, warning=FALSE, error=FALSE, echo=FALSE}
movies <- read.csv("data/movies.csv",stringsAsFactors=FALSE)
ratings <- read.csv("data/ratings.csv")
```

A summary of *movies* is given below, together with several first rows of a dataframe:
```{r mov_summ, warning=FALSE, error=FALSE, message=FALSE}
summary(movies)
head(movies)
```

And here is a summary and a head of *ratings*:
```{r rat_summ, warning=FALSE, error=FALSE, message=FALSE}
summary(ratings)
head(ratings)
```

## Data Pre-processing

### Extract a list of genres
Using __one-hot encoding__ to create a matrix of corresponding genres for each movie. This will help us to generate the list of recommendations for movies based on genres preferred by the user.
Even if we do not use the matrix to give genre based recommendation, we will be needing it to find similarities between users for the rating matrix.
```{r}
genres <- as.data.frame(movies$genres, stringsAsFactors=FALSE)

genres2 <- as.data.frame(tstrsplit(genres[,1], '[|]', 
                                   type.convert=TRUE), 
                         stringsAsFactors=FALSE)
colnames(genres2) <- c(1:10)

genre_list <- c("Action", "Adventure", "Animation", "Children", 
                "Comedy", "Crime","Documentary", "Drama", "Fantasy",
                "Film-Noir", "Horror", "Musical", "Mystery","Romance",
                "Sci-Fi", "Thriller", "War", "Western") # we have 18 genres in total

genre_matrix <- matrix(0,10330,18) #empty matrix, 10330=no of movies+1, 18=no of genres
genre_matrix[1,] <- genre_list #set first row to genre list
colnames(genre_matrix) <- genre_list #set column names to genre list

#iterate through matrix
for (i in 1:nrow(genres2)) {
  for (c in 1:ncol(genres2)) {
    genmat_col = which(genre_matrix[1,] == genres2[i,c])
    genre_matrix[i+1,genmat_col] <- 1
  }
}

 #remove first row, which was the genre list
genre_matrix2 <- as.data.frame(genre_matrix[-1,], stringsAsFactors=FALSE)
for (c in 1:ncol(genre_matrix2)) {
  genre_matrix2[,c] <- as.integer(genre_matrix2[,c])  #convert from characters to integers
} 

```
```{r rat_genre_matric, warning=FALSE, error=FALSE, message=FALSE}
summary(genre_matrix2)
head(genre_matrix2)
```


```{r search_genres, warning=FALSE, error=FALSE, echo=FALSE}
search_matrix <- cbind(movies[,1:2], genre_matrix2)
head(search_matrix)
```
It is seen from the ratings matrix above that each movie can correspond to one or more genres.

### Converting ratings matrix in a proper format

In order to use the ratings data for building a recommendation engine with *recommenderlab*, convert rating matrix into a sparse matrix of type *realRatingMatrix*.

```{r rat_mat, warning=FALSE, error=FALSE, echo=FALSE}
ratingmat <- dcast(ratings, userId~movieId, value.var = "rating", na.rm=FALSE)
ratingmat <- as.matrix(ratingmat[,-1]) #remove userIds

#Convert rating matrix into a recommenderlab sparse matrix
ratingmat <- as(ratingmat, "realRatingMatrix")
```
668 x 10325 rating matrix of class 'realRatingMatrix' with 105339 ratings.
<!-- ### Synchronize dimensions of matrices (???) -->

<!-- ```{r mat_dims, warning=FALSE, error=FALSE} -->
<!-- #Remove rows that are not rated from movies dataset -->
<!-- movieIds <- length(unique(movies$movieId)) #10329 -->
<!-- ratingmovieIds <- length(unique(ratings$movieId)) #10325 -->
<!-- movies2 <- movies[-which((movies$movieId %in% ratings$movieId) == FALSE),] -->
<!-- rownames(movies2) <- NULL -->

<!-- #Remove rows that are not rated from genre_matrix2 -->
<!-- genre_matrix3 <- genre_matrix2[-which((movies$movieId %in% ratings$movieId) == FALSE),] -->
<!-- rownames(genre_matrix3) <- NULL -->
<!-- ``` -->

## Explore values of movies ratings
```{r rate_values, warning=FALSE, error=FALSE}
vector_ratings <- as.vector(ratingmat@data)
unique(vector_ratings) # what are unique values of ratings

table_ratings <- table(vector_ratings) # what is the count of each rating value
table_ratings
```
There are 11 unique score values. The lower values mean lower ratings and vice versa.

### Distribution of the ratings

According to the documentation, a rating equal to 0 represents a missing value, hence it is removed from the dataset before visualizing the results.
```{r}
 # rating == 0 are NA values
vector_ratings <- vector_ratings[vector_ratings != 0]
vector_ratings <- factor(vector_ratings)

qplot(vector_ratings) + ggtitle("Distribution of the ratings")
```
There are less low (less than 3) rating scores, the majority of movies are rated with a score of 3 or higher. The most common rating is 4.

### Number of views of the top movies
```{r  top_no, warning=FALSE, error=FALSE, echo=FALSE}
views_per_movie <- colCounts(ratingmat) # count views for each movie

table_views <- data.frame(movie = names(views_per_movie),
                          views = views_per_movie) # create dataframe of views
table_views <- table_views[order(table_views$views, 
                                 decreasing = TRUE), ] # sort by number of views
table_views$title <- NA
for (i in 1:10325){
  table_views[i,3] <- as.character(subset(movies, 
                                         movies$movieId == table_views[i,1])$title)
}

table_views[1:6,]

ggplot(table_views[1:6, ], aes(x = title, y = views)) +
  geom_bar(stat="identity") + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
ggtitle("Number of views of the top movies")

```
From the plot, it can be seen that "Pulp Fiction (1994)" is the most viewed movie, exceeding the second-most-viewed "Forrest Gump (1994)" by 14 views.

### Distribution of the average movie rating
```{r avg_rat, warning=FALSE, error=FALSE, echo=FALSE, message=FALSE}
average_ratings <- colMeans(ratingmat)

qplot(average_ratings) + 
  stat_bin(binwidth = 0.1) +
  ggtitle("Distribution of the average movie rating")
```
The distribution above shows the distribution of the average movie rating. The highest value is around 3, and there are a few movies whose rating is either 1 or 5. Probably, the reason is that these movies received a rating from a few people only, so shouldn't take them into account.

Assigning a threshold value of minimum of 50 views per user, create a subset of only relevant movies. 
```{r}
average_ratings_relevant <- average_ratings[views_per_movie > 50] 
qplot(average_ratings_relevant) + 
  stat_bin(binwidth = 0.1) +
  ggtitle(paste("Distribution of the relevant average ratings"))
```

The second image above shows the distribution of the relevant average ratings. All the rankings are between 2.16 and 4.45. As expected, the extremes were removed. The highest value changes, and now it is around 4.


### Heatmap of the rating matrix
Visualizing the whole matrix of ratings by building a heat map whose colors represent the ratings. Each row of the matrix corresponds to a user, each column to a movie, and each cell to its rating.
```{r heat_rate, warning=FALSE, error=FALSE, echo=FALSE}
image(ratingmat, main = "Heatmap of the rating matrix")

```

Since there are too many users and items, the heatmap chart is hard to read hard as it has too many dimensions.

```{r}
image(ratingmat[1:20, 1:25], main = "Heatmap of the first 20 rows and 25 columns")
```
Zooming in on the first rows and columns, it can be observed that the some users saw more movies than the others.

To avoid this user bias, an efficient recommendation algorithm should select the most relevant users (the users who have seen many movies) and movies (the movies that have been seen by many users). To select the most relevant users and movies, the following steps are followed -

1. Determine the minimum number of movies per user.
2. Determine the minimum number of users per movie.
3. Select the users and movies matching these criteria.

```{r heat_relev, warning=FALSE, error=FALSE, echo=FALSE}
min_n_movies <- quantile(rowCounts(ratingmat), 0.99)
min_n_users <- quantile(colCounts(ratingmat), 0.99)
print("Minimum number of movies per user:")
min_n_movies
print("Minimum number of users per movie:")
min_n_users

image(ratingmat[rowCounts(ratingmat) > min_n_movies,
                 colCounts(ratingmat) > min_n_users], 
main = "Heatmap of the top users and movies")
```
From above heatmap the following points can be observed - 

* Of the users having watched more movies, most of them have seen all the top movies.
* Some columns of the heatmap are darker than the others, meaning that these columns represent the highest-rated movies.
* Conversely, darker rows represent users giving higher ratings.

Because of the above factors, it would be a good to normalize the data before building the model.

## Data Preparation

The data preparation process consists of the following steps:

1. Select the relevant data.
2. Normalize the data.
3. Binarize the data.

## Select the relevant data - 

In order to select the most relevant data, the minimum number of users per rated movie is defined as 50 and the minimum views number per movie as 50:

```{r rel_data, warning=FALSE, error=FALSE, echo=FALSE}
ratings_movies <- ratingmat[rowCounts(ratingmat) > 50,
                             colCounts(ratingmat) > 50]
```
420 x 447 rating matrix of class â€˜realRatingMatrixâ€™ with 38341 ratings.


Such a selection of the most relevant data contains 420 users and 447 movies, compared to previous 668 users and 10325 movies in the total dataset.

Using the same approach as previously, visualize the top 2 percent of users and movies in the new matrix of the most relevant data:

```{r}
min_movies <- quantile(rowCounts(ratings_movies), 0.98)
min_users <- quantile(colCounts(ratings_movies), 0.98)
image(ratings_movies[rowCounts(ratings_movies) > min_movies,
                     colCounts(ratings_movies) > min_users], 
main = "Heatmap of the top users and movies")

average_ratings_per_user <- rowMeans(ratings_movies)
qplot(average_ratings_per_user) + stat_bin(binwidth = 0.1) +
  ggtitle("Distribution of the average rating per user")
```

In the heatmap, some rows are darker than the others. This might mean that some users give higher ratings to all the movies. The distribution of the average rating per user across all the users varies a lot, as the second chart above shows.
    
## Normalizing the data -

Having users who give high (or low) ratings to all their movies might bias the results. In order to remove this effect, we normalize the data in such a way that the average rating of each user is 0.
```{r}
ratings_movies_norm <- normalize(ratings_movies)
sum(rowMeans(ratings_movies_norm) > 0.00001)
```

Visualizing the normalized matrix for the top movies. It is colored now because the data is continuous:
```{r viz_normal_data, warning=FALSE, error=FALSE, echo=FALSE}
image(ratings_movies_norm[rowCounts(ratings_movies_norm) > min_movies,
                          colCounts(ratings_movies_norm) > min_users], 
main = "Heatmap of the top users and movies")
```


There are still some lines that seem to be more blue or more red. The reason
is that I am visualizing only the top movies. We have already checked that the average rating is 0 for each user, so there is no bias in the user ratings.

## Binarizing the data
Some recommendation models work on binary data, so it might be useful to binarize the data, that is, define a table containing only 0s and 1s. The 0s will be either treated as missing values or as bad ratings.

We can either:

* Define a matrix having 1 if the user rated the movie, and 0 otherwise. In this case, the information about the rating is lost.

* Define a matrix having 1 if the rating is above or equal to a definite threshold (for example, 3), and 0 otherwise. In this case, giving a bad rating to a movie is equivalent to not having rated it.

Depending on the context, one choice may be more appropriate than the other.

As a next step, two matrices following the two different approaches are defined. Visualize a 5 percent portion of each of binarized matrices.

#### 1st option: define a matrix equal to 1 if the movie has been watched

```{r binar_data1, warning=FALSE, error=FALSE, echo=FALSE}
ratings_movies_watched <- binarize(ratings_movies, minRating = 1)
min_movies_binary <- quantile(rowCounts(ratings_movies), 0.95)
min_users_binary <- quantile(colCounts(ratings_movies), 0.95)
image(ratings_movies_watched[rowCounts(ratings_movies) > min_movies_binary,
                             colCounts(ratings_movies) > min_users_binary], 
main = "Heatmap of the top users and movies")
```

#### 2nd option: define a matrix equal to 1 if the cell has a rating above the threshold

```{r binar_data2, warning=FALSE, error=FALSE, echo=FALSE}
ratings_movies_good <- binarize(ratings_movies, minRating = 3)
image(ratings_movies_good[rowCounts(ratings_movies) > min_movies_binary, 
colCounts(ratings_movies) > min_users_binary], 
main = "Heatmap of the top users and movies")
```

There are more white cells in the second heatmap, which shows that there are more movies with no or bad ratings than those that were not watched by raters.

## ITEM-based Collaborative Filtering Model

Collaborative filtering is a branch of recommendation that takes account of the information about different users. The word "collaborative" refers to the fact that users collaborate with each other to recommend items. In fact, the algorithms take account of user ratings and preferences.

The starting point is a rating matrix in which rows correspond to users and columns correspond to items. The core algorithm is based on these steps:

1. For each two items, measure how similar they are in terms of having received similar ratings by similar users
2. For each item, identify the k most similar items
3. For each user, identify the items that are most similar to the user's purchases

## Defining training/test sets

We build the model using 80% of the whole dataset as a training set, and 20% - as a test set.

```{r train_test_sets, warning=FALSE, message=FALSE, echo=FALSE}
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(ratings_movies),
                      replace = TRUE, 
                      prob = c(0.8, 0.2))
#head(which_train)

recc_data_train <- ratings_movies[which_train, ]
recc_data_test <- ratings_movies[!which_train, ]

# which_set <- sample(x = 1:5, 
#                     size = nrow(ratings_movies), 
#                     replace = TRUE)
# for(i_model in 1:5) {
#   which_train <- which_set == i_model
#   recc_data_train <- ratings_movies[which_train, ]
#   recc_data_test <- ratings_movies[!which_train, ]
# }
```
## Building the recommendation model

A look at the default parameters of IBCF model. 

Here, *k* is the number of items to compute the similarities among them in the first step. After, for each item, the algorithm identifies its *k* most similar items and stores the number. *method* is a similarity function, which is *Cosine* by default, may also be *Pearson*. The recommender model is developed using the default parameters of method = Cosine and k=30.

```{r build_recommenderIBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType ="realRatingMatrix")
recommender_models$IBCF_realRatingMatrix$parameters

recc_model <- Recommender(data = recc_data_train, 
                          method = "IBCF",
                          parameter = list(k = 30))

recc_model
class(recc_model)
```

Exploring the recommender model:

```{r explore_IBCF, warning=FALSE, message=FALSE, echo=FALSE}
model_details <- getModel(recc_model)
#model_details$description
#model_details$k

class(model_details$sim) # this contains a similarity matrix
dim(model_details$sim)

n_items_top <- 20
image(model_details$sim[1:n_items_top, 1:n_items_top],
      main = "Heatmap of the first rows and columns")

row_sums <- rowSums(model_details$sim > 0)
table(row_sums)
col_sums <- colSums(model_details$sim > 0)
qplot(col_sums) + stat_bin(binwidth = 1) + ggtitle("Distribution of the column count")
```

*dgCMatrix* is a similarity matrix created by the model. Its dimensions are 447 x 447, which is equal to the number of items. The heatmap of 20 first items show that many values are equal to 0. The reason is that each row contains only k (30) elements that are greater than 0. The number of non-null elements for each column depends on how many times the corresponding movie was included in the top k of another movie. Thus, the matrix is not neccessarily simmetric, which is also the case in our model. 

The chart of the distribution of the number of elements by column shows there are a few movies that are similar to many others. 

## Applying recommender system on the dataset:

Now, it is possible to recommend movies to the users in the test set. I define
*n_recommended* equal to 10 that specifies the number of movies to recommend to each user.

For each user, the algorithm extracts its rated movies. For each movie, it identifies all its similar items, starting from the similarity matrix. Then, the algorithm ranks each similar item in this way:

* Extract the user rating of each purchase associated with this item. The rating is used as a weight.
* Extract the similarity of the item with each purchase associated with this item.
* Multiply each weight with the related similarity. 
* Sum everything up.

Then, the algorithm identifies the top 10 recommendations:

```{r apply_IBCF, warning=FALSE, message=FALSE, echo=FALSE}
n_recommended <- 10 # the number of items to recommend to each user

recc_predicted <- predict(object = recc_model, 
                          newdata = recc_data_test, 
                          n = n_recommended)
##recc_predicted
```

Let's explore the results of the recommendations for the first user:

```{r explore_res_IBCF, warning=FALSE, message=FALSE, echo=FALSE}
#class(recc_predicted)
#slotNames(recc_predicted)

recc_user_1 <- recc_predicted@items[[1]] # recommendation for the first user
movies_user_1 <- recc_predicted@itemLabels[recc_user_1]
movies_user_2 <- movies_user_1
for (i in 1:10){
  movies_user_2[i] <- as.character(subset(movies, 
                                         movies$movieId == movies_user_1[i])$title)
}
movies_user_2
```

It's also possible to define a matrix with the recommendations for each user. 
Below we visualize the recommendations for the first four users:

```{r recc_matrix, warning=FALSE, message=FALSE, echo=FALSE}
recc_matrix <- sapply(recc_predicted@items, 
                      function(x){ as.integer(colnames(ratings_movies)[x]) }) # matrix with the recommendations for each user
#dim(recc_matrix)
recc_matrix[,1:4]
```

Here, the columns represent the first 4 users, and the rows are the *movieId* values of recommended 10 movies.

Now, let's identify the most recommended movies. The following image shows the distribution of the number of items for IBCF:

```{r most_recom_moviesIBCF, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items <- factor(table(recc_matrix))

chart_title <- "Distribution of the number of items for IBCF"
qplot(number_of_items) + ggtitle(chart_title)

number_of_items_sorted <- sort(number_of_items, decreasing = TRUE)
number_of_items_top <- head(number_of_items_sorted, n = 4)
table_top <- data.frame(as.integer(names(number_of_items_top)),
                       number_of_items_top)

for (i in 1:4){
  table_top[i,1] <- as.character(subset(movies, 
                                         movies$movieId == table_top[i,1])$title)
}

colnames(table_top) <- c("Movie title", "No of items")
head(table_top)
```

Most of the movies have been recommended only a few times, and a few movies have been recommended more than 5 times.

IBCF recommends items on the basis of the similarity matrix. It's an eager-learning model, that is, once it's built, it doesn't need to access the initial data. For each item, the model stores the k-most similar, so the amount of information is small once the model is built. This is an advantage in the presence of lots of data.

In addition, this algorithm is efficient and scalable, so it works well with big rating matrices.

## USER-based Collaborative Filtering Model

Now, we will use the user-based approach to develop a recommender engine. According to this approach, given a new user, its similar users are first identified. Then, the top-rated items rated by
similar users are recommended. 

For each new user, these are the steps:

1. Measure how similar each user is to the new one. Like IBCF, popular similarity measures are correlation and cosine.
2. Identify the most similar users. The options are:

   * Take account of the top k users (k-nearest_neighbors)
   * Take account of the users whose similarity is above a defined threshold
   
3. Rate the movies rated by the most similar users. The rating is the average
rating among similar users and the approaches are:

   * Average rating
   * Weighted average rating, using the similarities as weights
   
4. Pick the top-rated movies.


## Building the recommendation system:

First check the default parameters of UBCF model. Here, *nn* is a number of similar users, and *method* is a similarity function, which is *cosine* by default. We will build a recommender model leaving the parameters to their defaults and using the training set.

```{r build_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType ="realRatingMatrix")
recommender_models$UBCF_realRatingMatrix$parameters
recc_model <- Recommender(data = recc_data_train, method = "UBCF")
recc_model
model_details <- getModel(recc_model)
model_details$data
```
Recommender of type â€˜UBCFâ€™ for â€˜realRatingMatrixâ€™ 
learned using 332 users.
332 x 447 rating matrix of class â€˜realRatingMatrixâ€™ with 29519 ratings.
Normalized using center on rows.

## Applying the recommender model on the test set

In the same way as the IBCF, we now determine the top ten recommendations for each new user in the test set.
```{r apply_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
n_recommended <- 10
recc_predicted <- predict(object = recc_model,
                          newdata = recc_data_test, 
                          n = n_recommended) 
recc_predicted
```
## Explore results

Let's take a look at the first four users:
```{r explore_UBCF}
recc_matrix <- sapply(recc_predicted@items, 
                      function(x){ as.integer(colnames(ratings_movies)[x]) })
recc_matrix[, 1:4]
```
The above matrix contain *movieId* of each recommended movie (rows) for the first four users (columns) in our test dataset.

We now compute how many times each movie got recommended and build the related frequency histogram:
```{r}
number_of_items <- factor(table(recc_matrix))

chart_title <- "Distribution of the number of items for UBCF"
qplot(number_of_items) + ggtitle(chart_title)
```

Compared with the IBCF, the distribution has a longer tail. This means that there are some movies that are recommended much more often than the others. The maximum is more than 30, compared to 10-ish for IBCF.

```{r top_titles_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items_sorted <- sort(number_of_items, decreasing = TRUE)
number_of_items_top <- head(number_of_items_sorted, n = 4)
table_top <- data.frame(as.integer(names(number_of_items_top)), number_of_items_top)

for (i in 1:4){
  table_top[i,1] <- as.character(subset(movies, 
                                         movies$movieId == table_top[i,1])$title)
}
colnames(table_top) <- c("Movie title", "No of items")
head(table_top)
```

Comparing the results of UBCF with IBCF helps find some useful insight on different algorithms. UBCF needs to access the initial data. Since it needs to keep the entire database in memory, it doesn't work well in the presence of a big rating matrix. Also, building the similarity matrix requires a lot of computing power and time.

However, UBCF's accuracy is proven to be slightly more accurate than IBCF (refer to next section), so it's a good option if the dataset is not too big.

## Evaluating the Recommender Systems

There are a few options to choose from when deciding to create a recommendation engine. In order to compare their performances and choose the most appropriate model, we follow these steps:

* Prepare the data to evaluate performance
* Evaluate the performance of some models
* Choose the best performing models
* Optimize model parameters

## Preparing the data to evaluate models

We need two trainig and testing data to evaluate the model. There are several methods to create them: 1) splitting the data into training and test sets, 2) bootstrapping, 3) using k-fold.

### Splitting the data

Splitting the data into training and test sets is often done using a 80/20 proportion.
```{r  eval_split, warning=FALSE, message=FALSE, echo=FALSE}
percentage_training <- 0.8
```
For each user in the test set, we need to define how many items to use to generate
recommendations. For this, first check the minimum number of items rated by users to be sure there will be no users with no items to test.

```{r  split_parameters, message=FALSE, warning=FALSE}
items_to_keep <- 5 #number of items to generate recommendations
rating_threshold <- 3 # threshold with the minimum rating that is considered good
n_eval <- 1 #number of times to run evaluation

eval_sets <- evaluationScheme(data = ratings_movies, 
                              method = "split",
                              train = percentage_training, 
                              given = items_to_keep, 
                              goodRating = rating_threshold, 
                              k = n_eval) 
eval_sets

getData(eval_sets, "train") # training set
getData(eval_sets, "known") # set with the items used to build the recommendations
getData(eval_sets, "unknown") # set with the items used to test the recommendations

qplot(rowCounts(getData(eval_sets, "unknown"))) + 
  geom_histogram(binwidth = 10) + 
  ggtitle("unknown items by the users")
```
The above image displays the unknown items by the users, which varies a lot.

### Bootstrapping the data
Bootstrapping is another approach to split the data. The same user can be sampled more than once and, if the training set has the same size as it did earlier, 
there will be more users in the test set.

```{r bootstrap, message=FALSE, warning=FALSE}
eval_sets <- evaluationScheme(data = ratings_movies, 
                              method = "bootstrap", 
                              train = percentage_training, 
                              given = items_to_keep,
                              goodRating = rating_threshold, 
                              k = n_eval)

table_train <- table(eval_sets@runsTrain[[1]])
n_repetitions <- factor(as.vector(table_train))
qplot(n_repetitions) + 
  ggtitle("Number of repetitions in the training set")
```
The above chart shows that most of the users have been sampled fewer than four times.

### Using cross-validation to validate models

The k-fold cross-validation approach is the most accurate one, although it's computationally heavier.

Using this approach, we split the data into some chunks, take a chunk out as the test set, and evaluate the accuracy. Then, we can do the same with each other chunk and compute the average accuracy.

```{r k-fold, message=FALSE, warning=FALSE}
n_fold <- 4
eval_sets <- evaluationScheme(data = ratings_movies, 
                              method = "cross-validation",
                              k = n_fold, 
                              given = items_to_keep, 
                              goodRating = rating_threshold)
size_sets <- sapply(eval_sets@runsTrain, length)
size_sets
```

Using 4-fold approach, we get four sets of the same size 315.

## Evaluating the ratings

We use the k-fold approach for evaluation. 

First, re-define the evaluation sets, build IBCF model and create a matrix with predicted ratings.

```{r eval_ratings, message=FALSE, warning=FALSE, echo=FALSE}
eval_sets <- evaluationScheme(data = ratings_movies, 
                              method = "cross-validation",
                              k = n_fold, 
                              given = items_to_keep, 
                              goodRating = rating_threshold)

model_to_evaluate <- "IBCF"
model_parameters <- NULL

eval_recommender <- Recommender(data = getData(eval_sets, "train"),
                                method = model_to_evaluate, 
                                parameter = model_parameters)

items_to_recommend <- 10
eval_prediction <- predict(object = eval_recommender, 
                           newdata = getData(eval_sets, "known"), 
                           n = items_to_recommend, 
                           type = "ratings")

qplot(rowCounts(eval_prediction)) + 
  geom_histogram(binwidth = 10) +
  ggtitle("Distribution of movies per user")
```

The above image displays the distribution of movies per user in the matrix of predicted ratings.

Now, compute the accuracy measures for each user. Most of the RMSEs (Root mean square errors) are in the range of 0.5 to 1.8:

```{r acc, message=FALSE,  warning=FALSE, echo=FALSE}
eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = TRUE)
head(eval_accuracy)

qplot(eval_accuracy[, "RMSE"]) + 
  geom_histogram(binwidth = 0.1) +
  ggtitle("Distribution of the RMSE by user")
```

In order to have a performance index for the whole model, specify *byUser* as FALSE and compute the average indices:

```{r acc_IBCF, message=FALSE,  warning=FALSE, echo=FALSE}
eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = FALSE) 
eval_accuracy
```

The measures of accuracy are useful to compare the performance of different models on the same data.

## Evaluating the recommendations

Another way to measure accuracies is by comparing the recommendations with
the purchases having a positive rating. For this, we can make use of a prebuilt
*evaluate* function in *recommenderlab* library. The function evaluate the recommender performance depending on the number *n* of items to recommend to each user. Use *n* as a sequence n = seq(10, 100, 10). The first rows of the resulting performance matrix is presented below:

```{r eval_recomms, message=FALSE, warning=FALSE, echo=FALSE}
results <- evaluate(x = eval_sets, 
                    method = model_to_evaluate, 
                    n = seq(10, 100, 10))

head(getConfusionMatrix(results)[[1]])
```

In order to have a look at all the splits at the same time, sum up the indices of columns TP, FP, FN and TN:

```{r conf_matrix_whole, message=FALSE, warning=FALSE, echo=FALSE}
columns_to_sum <- c("TP", "FP", "FN", "TN")
indices_summed <- Reduce("+", getConfusionMatrix(results))[, columns_to_sum]
head(indices_summed)
```

## Comparing models

In order to compare different models, I define them as a following list:

* Item-based collaborative filtering, using the Cosine as the distance function
* Item-based collaborative filtering, using the Pearson correlation as the distance function
* User-based collaborative filtering, using the Cosine as the distance function
* User-based collaborative filtering, using the Pearson correlation as the distance function
* Random recommendations to have a base line

```{r define_diff_models, warning=FALSE, message=FALSE, echo=FALSE}
models_to_evaluate <- list(
IBCF_cos = list(name = "IBCF", 
                param = list(method = "cosine")),
IBCF_cor = list(name = "IBCF", 
                param = list(method = "pearson")),
UBCF_cos = list(name = "UBCF", 
                param = list(method = "cosine")),
UBCF_cor = list(name = "UBCF", 
                param = list(method = "pearson")),
random = list(name = "RANDOM", param=NULL)
)
```

Then, I define a different set of numbers for recommended movies (n_recommendations <- c(1, 5, seq(10, 100, 10))), run and evaluate the models:

```{r params, warning=FALSE, message=FALSE, echo=FALSE}
n_recommendations <- c(1, 5, seq(10, 100, 10))
list_results <- evaluate(x = eval_sets, 
                         method = models_to_evaluate, 
                         n = n_recommendations)

sapply(list_results, class) == "evaluationResults"
```

The following table presents as an example the first rows of the performance evaluation matrix for the IBCF with Cosine distance:

```{r ex_compare, warning=FALSE, message=FALSE, echo=FALSE}
avg_matrices <- lapply(list_results, avg)
head(avg_matrices$IBCF_cos[, 5:8])
```

## Identifying the most suitable model

I compare the models by building a chart displaying their ROC curves and Precision/recall curves.

```{r compare_models_roc, message=FALSE, warning=FALSE, echo=FALSE}
plot(list_results, annotate = 1, legend = "topleft") 
title("ROC curve")

plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
```

A good performance index is the area under the curve (AUC), that is, the area under
the ROC curve. Even without computing it, the chart shows that the highest is UBCF
with cosine distance, so it's the best-performing technique.

The UBCF with cosine distance is still the top model. Depending on what is the main purpose of the system, an appropriate number of items to recommend should be defined.

## Optimizing a numeric parameter

IBCF takes account of the k-closest items. I will explore more values, ranging
between 5 and 40, in order to tune this parameter:

```{r optimize, message=FALSE, warning=FALSE}
vector_k <- c(5, 10, 20, 30, 40)
models_to_evaluate <- lapply(vector_k, function(k){
  list(name = "IBCF",
       param = list(method = "cosine", k = k))
})
names(models_to_evaluate) <- paste0("IBCF_k_", vector_k)
```

Now I build and evaluate the same IBCF/cosine models with different values of the k-closest items:

```{r eval_optimized, message=FALSE, warning=FALSE, echo=FALSE}
n_recommendations <- c(1, 5, seq(10, 100, 10))
list_results <- evaluate(x = eval_sets, 
                         method = models_to_evaluate, 
                         n = n_recommendations)

plot(list_results, annotate = 1, legend = "topleft") 
title("ROC curve")

plot(list_results, "prec/rec", annotate = 1, legend = "bottomright")
title("Precision-recall")
```

Based on the ROC curve's plot, the k having the biggest AUC is 10. Another good candidate is 5, but it can never have a high TPR. This means that, even if we set a very high n value, the algorithm won't be able to recommend a big percentage of items that the user liked. The IBCF with k = 5 recommends only a few items similar to the purchases. Therefore, it can't be used to recommend many items.

Based on the precision/recall plot, k should be set to 10 to achieve the highest recall. If we are more interested in the precision, we set k to 5.